version: '3.7'

services:
  redis:
    image: mirror.gcr.io/valkey/valkey:8.1-alpine
    command: valkey-server --appendonly yes
    healthcheck:
      test: valkey-cli ping
    restart: unless-stopped
    volumes:
      - ./redis/data:/data
    logging: &logging
      driver: journald
      options:
        tag: '{{.Name}}'

  db:
    image: postgres:14.0-alpine
    healthcheck:
      test: pg_isready -U ${POSTGRES_USER} || exit 1
    restart: unless-stopped
    env_file: ./.env
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - ./db/data:/var/lib/postgresql/data
    logging:
      <<: *logging

  backups:
    build:
      context: backups/
      dockerfile: Dockerfile
    init: true
    restart: unless-stopped
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - BACKUP_LOCAL_DIR=/var/backups
      - BACKUP_LOCAL_ROTATE_KEEP_LAST=${BACKUP_LOCAL_ROTATE_KEEP_LAST}
      - B2_BUCKET=${BACKUP_B2_BUCKET}
      - B2_FOLDER=${BACKUP_B2_FOLDER}
      - B2_APPLICATION_KEY_ID=${BACKUP_B2_APPLICATION_KEY_ID}
      - B2_APPLICATION_KEY=${BACKUP_B2_APPLICATION_KEY}
      - EMAIL_HOST=${EMAIL_HOST}
      - EMAIL_PORT=${EMAIL_PORT}
      - EMAIL_HOST_USER=${EMAIL_HOST_USER}
      - EMAIL_HOST_PASSWORD=${EMAIL_HOST_PASSWORD}
      - EMAIL_TARGET=${EMAIL_TARGET}
      - DEFAULT_FROM_EMAIL=${DEFAULT_FROM_EMAIL}
      - SENTRY_DSN=${SENTRY_DSN}
    volumes:
      - backups:/var/backups
    depends_on:
      - db
    logging:
      <<: *logging

  app:
    build:
      context: .
      dockerfile: app/envs/prod/Dockerfile
    image: project/app
    healthcheck:
      test: ["CMD", "./healthcheck.py", "/var/run/gunicorn/gunicorn.sock"]
    init: true
    restart: unless-stopped
    env_file: ./.env
    environment:
      # Add this variable to all containers that should dump Prometheus metrics.  Each container besides this one
      # should use a different subdirectory of /prometheus-multiproc-dir, e.g.
      # - PROMETHEUS_MULTIPROC_DIR=/prometheus-multiproc-dir/other-container
      # Don't forget to also mount the prometheus-metrics volume in other containers too.
      - PROMETHEUS_MULTIPROC_DIR=/prometheus-multiproc-dir
      - MEDIA_ROOT=${MEDIA_ROOT:-/root/src/media}
    volumes:
      - backend-static:/root/src/static
      - gunicorn-socket:/var/run/gunicorn
      - ./media:/root/src/media
      # Add this mount to each container that should dump Prometheus metrics.
      - ./prometheus-metrics:/prometheus-multiproc-dir
    depends_on:
      - redis
      - db
    logging:
      <<: *logging

  block-scheduler:
    build:
      context: .
      dockerfile: app/envs/prod/Dockerfile
    image: block_dumper/app
    init: true
    restart: unless-stopped
    env_file: ./.env
    command: ./block-scheduler-entrypoint.sh
    environment:
      - PROMETHEUS_MULTIPROC_DIR=/prometheus-multiproc-dir
      - SENTINEL_MODE=${SENTINEL_MODE:-live}
      - BLOCK_START=${BLOCK_START:-}
      - BLOCK_END=${BLOCK_END:-}
      - BACKFILL_RATE_LIMIT=${BACKFILL_RATE_LIMIT:-1.0}
    volumes:
      - backend-static:/root/src/static
      - ./media:/root/src/media
      - ./prometheus-metrics:/prometheus-multiproc-dir
    depends_on:
      - celery-worker
      - redis
      - db
    logging:
      <<: *logging

  celery-worker:
    image: project/app
    init: true
    healthcheck:
      test: celery -A project status > /dev/null || exit 1
    restart: unless-stopped
    env_file: ./.env
    environment:
      - DEBUG=off
      - PROMETHEUS_MULTIPROC_DIR=/prometheus-multiproc-dir/celery-worker
      - MEDIA_ROOT=${MEDIA_ROOT:-/root/src/media}
      - SENTINEL_MODE=${SENTINEL_MODE:-live}
    command: ./celery-entrypoint.sh
    volumes:
      - ./media:/root/src/media
      - ./prometheus-metrics:/prometheus-multiproc-dir
    tmpfs: /run
    depends_on:
      - redis
    logging:
      <<: *logging

  celery-beat:
    image: project/app
    init: true
    restart: unless-stopped
    env_file: ./.env
    environment:
      - DEBUG=off
    command: nice celery -A project beat -l INFO --schedule /tmp/celerybeat-schedule -f /tmp/logs/celery-beat.log
    volumes:
      - ./logs:/tmp/logs
    depends_on:
      - redis
    logging:
      <<: *logging

  celery-flower:
    image: project/app
    healthcheck:
      test: wget --user "${CELERY_FLOWER_USER}" --password "${CELERY_FLOWER_PASSWORD}" -qO- 127.0.0.1:5555 > /dev/null || exit 1
    init: true
    restart: unless-stopped
    env_file: ./.env
    environment:
      - DEBUG=off
      - FLOWER_TASK_RUNTIME_METRIC_BUCKETS=1,2,3,5,10,20,30,45,60,120,180,240,300,600,inf
    command: celery --app=project --broker="${CELERY_BROKER_URL}" flower --basic_auth="${CELERY_FLOWER_USER}:${CELERY_FLOWER_PASSWORD}"
    depends_on:
      - celery-worker
    ports:
      - 127.0.0.1:5555:5555
    logging:
      <<: *logging

  dagster:
    image: project/app
    healthcheck:
      test: wget -qO- 127.0.0.1:3000/dagster/server_info > /dev/null || exit 1
    init: true
    restart: unless-stopped
    env_file: ./.env
    environment:
      - DEBUG=off
      - DAGSTER_HOME=/root/src
      - MEDIA_ROOT=${MEDIA_ROOT:-/root/src/media}
    command: ./dagster-entrypoint.sh
    volumes:
      - ./media:/root/src/media
      - dagster-storage:/root/src/.dagster
    depends_on:
      - redis
      - db
    ports:
      - 127.0.0.1:3000:3000
    logging:
      <<: *logging

  dagster-daemon:
    image: project/app
    init: true
    restart: unless-stopped
    env_file: ./.env
    environment:
      - DEBUG=off
      - DAGSTER_HOME=/root/src
      - MEDIA_ROOT=${MEDIA_ROOT:-/root/src/media}
    command: dagster-daemon run -m project.dagster.definitions
    volumes:
      - ./media:/root/src/media
      - dagster-storage:/root/src/.dagster
    depends_on:
      - dagster
    logging:
      <<: *logging

  nginx:
    image: 'ghcr.io/reef-technologies/nginx-rt:v1.2.2'
    restart: unless-stopped
    healthcheck:
      test: [
        "CMD-SHELL",
        "curl 0.0.0.0:80/alive/ -s --fail -H \"Host: $NGINX_HOST\" -H \"User-Agent: docker-compose-healthcheck\" -o /dev/null || exit 1"
      ]
      interval: 30s
      retries: 5
      start_period: 20s
      timeout: 10s
    environment:
      - NGINX_HOST=${NGINX_HOST}
    volumes:
      - ./nginx/templates:/etc/nginx/templates
      - ./nginx/config_helpers:/etc/nginx/config_helpers
      - backend-static:/srv/static:ro
      - ./media:/srv/media:ro
      - ./letsencrypt/etc:/etc/letsencrypt
      - ./nginx/monitoring_certs:/etc/monitoring_certs
      - gunicorn-socket:/var/run/gunicorn:ro
    depends_on:
      - app
      - cadvisor
      - node-exporter
      - dagster
      - grafana
    command: nginx -g 'daemon off;'
    ports:
      - 80:80
      - 443:443
      - 10443:10443
    logging:
      <<: *logging
    extra_hosts:
      - "host.docker.internal:host-gateway"

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    network_mode: host
    pid: host
    volumes:
      - /:/host:ro,rslave
      - nodeexporter_collectors:/textfile_collectors

    command:
      - '--path.rootfs=/host'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc|run|boot|var/.+)($$|/)'
      - '--collector.textfile.directory=textfile_collectors'
      - '--collector.tcpstat'
    logging:
      <<: *logging

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.40.0
    container_name: cadvisor
    devices:
      - /dev/kmsg:/dev/kmsg
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /cgroup:/cgroup:ro
    restart: unless-stopped
    logging:
      <<: *logging

  alloy:
    image: grafana/alloy:v1.8.3
    restart: unless-stopped
    environment:
      - LOKI_URL=${LOKI_URL}
      - LOKI_USER=${LOKI_USER}
      - LOKI_PASSWORD=${LOKI_PASSWORD}
    volumes:
      - ./alloy:/etc/alloy
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - alloy-data:/var/lib/alloy
    command:
      - run
      - --storage.path=/var/lib/alloy/data
      - /etc/alloy/config.alloy
    logging:
      <<: *logging
  vulnrelay:
    image: 'ghcr.io/reef-technologies/vulnrelay:latest'
    container_name: vulnrelay
    restart: unless-stopped
    env_file: ./.vuln.env
    environment:
      - METRICS_DIR=/app/metrics
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - nodeexporter_collectors:/app/metrics
    logging:
      <<: *logging
  watchtower:
    image: containrrr/watchtower:latest
    restart: unless-stopped
    environment:
      - DOCKER_API_VERSION=1.45
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: --interval 7200 vulnrelay
    logging:
      <<: *logging

  pylon:
    image: docker.io/backenddevelopersltd/bittensor-pylon:git-169a0e490aa92b7d0ca6392d65eb0d322c5b700c
    platform: linux/amd64
    restart: unless-stopped
    env_file: ./.env
    ports:
      - 127.0.0.1:${PYLON_PORT:-8001}:8000
    volumes:
      - ~/.bittensor/wallets/:/root/.bittensor/wallets
    logging:
      <<: *logging

  prometheus:
    image: prom/prometheus:v2.47.0
    restart: unless-stopped
    volumes:
      - ./prometheus:/etc/prometheus:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - 127.0.0.1:9090:9090
    logging:
      <<: *logging

  alertmanager:
    image: prom/alertmanager:v0.26.0
    restart: unless-stopped
    volumes:
      - ./prometheus/alertmanager.yml:/etc/alertmanager/alertmanager.yml.template:ro
    entrypoint: /bin/sh
    command:
      - -c
      - |
        cat /etc/alertmanager/alertmanager.yml.template | sed "s|\$${SLACK_WEBHOOK_URL}|$$SLACK_WEBHOOK_URL|g" > /etc/alertmanager/alertmanager.yml
        exec /bin/alertmanager --config.file=/etc/alertmanager/alertmanager.yml --storage.path=/alertmanager
    environment:
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}
    ports:
      - 127.0.0.1:9093:9093
    logging:
      <<: *logging

  grafana:
    image: grafana/grafana:10.2.0
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=https://${NGINX_HOST}/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_UNIFIED_ALERTING_ENABLED=true
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - DISCORD_SUDO_ALERTS_WEBHOOK_URL=${DISCORD_SUDO_ALERTS_WEBHOOK_URL}
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
      - db
    logging:
      <<: *logging

volumes:
  backend-static:
  backups:
  gunicorn-socket:
  nodeexporter_collectors:
  alloy-data:
  dagster-storage:
  prometheus-data:
  grafana-data:
